{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import argrelextrema\n",
    "import talib\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e4903\\AppData\\Local\\Temp\\ipykernel_1608\\586765529.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Trend'].iloc[i:i + trend_days] = 0\n",
      "C:\\Users\\e4903\\AppData\\Local\\Temp\\ipykernel_1608\\586765529.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Trend'].iloc[i:i + trend_days] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Open                   0\n",
       "High                   0\n",
       "Low                    0\n",
       "Close                  0\n",
       "Adj Close              0\n",
       "Volume                 0\n",
       "Trend                 19\n",
       "MACD                  17\n",
       "ROC                    5\n",
       "StoK                   8\n",
       "StoD                   8\n",
       "CCI                   13\n",
       "RSI                   14\n",
       "VMA                   19\n",
       "pctChange              1\n",
       "3M Treasury Yield      7\n",
       "5Y Treasury Yield      7\n",
       "10Y Treasury Yield     7\n",
       "30Y Treasury Yield     7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Define the Product interface\n",
    "class Indicator(ABC):\n",
    "    @abstractmethod\n",
    "    def compute(self, data=None, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "# todo: there's some wrong in calculate trend ways\n",
    "# Step 2: Implement Concrete Products\n",
    "\n",
    "\n",
    "class TrendIndicator(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        method = kwargs.get('method', 'MA')\n",
    "        ma_days = kwargs.get('ma_days', 20)\n",
    "        oder_days = kwargs.get('oder_days', 20)\n",
    "        trend_days = kwargs.get('trend_days', 5)\n",
    "\n",
    "        if method == 'MA':\n",
    "            return self.calculate_trend_MA(data, ma_days=ma_days, trend_days=trend_days)\n",
    "        elif method == 'LocalExtrema':\n",
    "            return self.calculate_trend_LocalExtrema(data, oder_days=oder_days)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid trend calculation method: {method}\")\n",
    "\n",
    "    def calculate_trend_MA(self, data, ma_days=20, trend_days=5):\n",
    "        data['MA'] = data['Close'].rolling(window=ma_days).mean()\n",
    "        data['Trend'] = np.nan\n",
    "        n = len(data)\n",
    "\n",
    "        for i in range(n - trend_days + 1):\n",
    "            if all(data['MA'].iloc[i + j] < data['MA'].iloc[i + j + 1] for j in range(trend_days - 1)):\n",
    "                data['Trend'].iloc[i:i + trend_days] = 0\n",
    "            elif all(data['MA'].iloc[i + j] > data['MA'].iloc[i + j + 1] for j in range(trend_days - 1)):\n",
    "                data['Trend'].iloc[i:i + trend_days] = 1\n",
    "        data['Trend'].fillna(method='ffill', inplace=True)\n",
    "        return data.drop(columns=['MA'])\n",
    "\n",
    "    def calculate_trend_LocalExtrema(self, data, oder_days=20):\n",
    "        local_max_indices = argrelextrema(\n",
    "            data['Close'].values, np.greater_equal, order=oder_days)[0]\n",
    "        local_min_indices = argrelextrema(\n",
    "            data['Close'].values, np.less_equal, order=oder_days)[0]\n",
    "        data['Local Max'] = data.iloc[local_max_indices]['Close']\n",
    "        data['Local Min'] = data.iloc[local_min_indices]['Close']\n",
    "        data['Trend'] = np.nan\n",
    "        prev_idx = None\n",
    "        prev_trend = None\n",
    "        prev_type = None\n",
    "\n",
    "        for idx in sorted(np.concatenate([local_max_indices, local_min_indices])):\n",
    "            if idx in local_max_indices:\n",
    "                current_type = \"max\"\n",
    "            else:\n",
    "                current_type = \"min\"\n",
    "\n",
    "            if prev_trend is None:\n",
    "                if current_type == \"max\":\n",
    "                    prev_trend = 1\n",
    "                else:\n",
    "                    prev_trend = 0\n",
    "            else:\n",
    "                if prev_type == \"max\" and current_type == \"min\":\n",
    "                    data.loc[prev_idx:idx, 'Trend'] = 1\n",
    "                    prev_trend = 1\n",
    "                elif prev_type == \"min\" and current_type == \"max\":\n",
    "                    data.loc[prev_idx:idx, 'Trend'] = 0\n",
    "                    prev_trend = 0\n",
    "                else:\n",
    "                    if current_type == \"max\":\n",
    "                        data.loc[prev_idx:idx, 'Trend'] = 0\n",
    "                        prev_trend = 0\n",
    "                    else:\n",
    "                        data.loc[prev_idx:idx, 'Trend'] = 1\n",
    "                        prev_trend = 1\n",
    "\n",
    "            prev_idx = idx\n",
    "            prev_type = current_type\n",
    "        data['Trend'].fillna(method='ffill', inplace=True)\n",
    "        return data.drop(columns=['Local Max', 'Local Min'])\n",
    "\n",
    "\n",
    "class MACDIndicator(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        fastperiod = kwargs.get('fastperiod', 5)\n",
    "        slowperiod = kwargs.get('slowperiod', 10)\n",
    "        signalperiod = kwargs.get('signalperiod', 9)\n",
    "        data['MACD'], _, _ = talib.MACD(\n",
    "            data['Close'], fastperiod=fastperiod, slowperiod=slowperiod, signalperiod=signalperiod)\n",
    "        return data\n",
    "\n",
    "\n",
    "class ROCIndicator(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        trend_days = kwargs.get('trend_days', 5)\n",
    "        data['ROC'] = talib.ROC(data['Close'], timeperiod=trend_days)\n",
    "        return data\n",
    "\n",
    "# Step 2: Implement Concrete Products (Continued)\n",
    "\n",
    "\n",
    "class StochasticOscillatorIndicator(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        trend_days = kwargs.get('trend_days', 5)\n",
    "        data['StoK'], data['StoD'] = talib.STOCH(\n",
    "            data['High'], data['Low'], data['Close'], fastk_period=trend_days, slowk_period=3, slowd_period=3)\n",
    "        return data\n",
    "\n",
    "\n",
    "class CCIIndicator(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        timeperiod = kwargs.get('timeperiod', 14)\n",
    "        data['CCI'] = talib.CCI(data['High'], data['Low'],\n",
    "                                data['Close'], timeperiod=timeperiod)\n",
    "        return data\n",
    "\n",
    "\n",
    "class RSIIndicator(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        timeperiod = kwargs.get('timeperiod', 14)\n",
    "        data['RSI'] = talib.RSI(data['Close'], timeperiod=timeperiod)\n",
    "        return data\n",
    "\n",
    "\n",
    "class VMAIndicator(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        timeperiod = kwargs.get('timeperiod', 20)\n",
    "        data['VMA'] = talib.MA(data['Volume'], timeperiod=timeperiod)\n",
    "        return data\n",
    "\n",
    "\n",
    "class PctChangeIndicator(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        data['pctChange'] = data['Close'].pct_change() * 100\n",
    "        return data\n",
    "\n",
    "\n",
    "class ThreeMonthTreasuryYield(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        start_date = kwargs.get('start_date')\n",
    "        end_date = kwargs.get('end_date')\n",
    "        three_month_treasury_yield = yf.download(\n",
    "            \"^IRX\", start_date, end_date)[\"Close\"]\n",
    "        data['3M Treasury Yield'] = three_month_treasury_yield\n",
    "        return data\n",
    "\n",
    "\n",
    "class FiveYearTreasuryYield(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        start_date = kwargs.get('start_date')\n",
    "        end_date = kwargs.get('end_date')\n",
    "        five_year_treasury_yield = yf.download(\n",
    "            \"^FVX\", start_date, end_date)[\"Close\"]\n",
    "        data['5Y Treasury Yield'] = five_year_treasury_yield\n",
    "        return data\n",
    "\n",
    "\n",
    "class TenYearTreasuryYield(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        start_date = kwargs.get('start_date')\n",
    "        end_date = kwargs.get('end_date')\n",
    "        ten_year_treasury_yield = yf.download(\n",
    "            \"^TNX\", start_date, end_date)[\"Close\"]\n",
    "        data['10Y Treasury Yield'] = ten_year_treasury_yield\n",
    "        return data\n",
    "\n",
    "\n",
    "class ThirtyYearTreasuryYield(Indicator):\n",
    "    def compute(self, data, *args, **kwargs):\n",
    "        start_date = kwargs.get('start_date')\n",
    "        end_date = kwargs.get('end_date')\n",
    "        thirty_year_treasury_yield = yf.download(\n",
    "            \"^TYX\", start_date, end_date)[\"Close\"]\n",
    "        data['30Y Treasury Yield'] = thirty_year_treasury_yield\n",
    "        return data\n",
    "\n",
    "# Add other indicators here as needed\n",
    "\n",
    "\n",
    "# Step 3: Define the Factory\n",
    "class IndicatorFactory:\n",
    "    @staticmethod\n",
    "    def get_indicator(indicator_type):\n",
    "        indicators = {\n",
    "            \"Trend\": TrendIndicator,\n",
    "            \"MACD\": MACDIndicator,\n",
    "            \"ROC\": ROCIndicator,\n",
    "            \"Stochastic Oscillator\": StochasticOscillatorIndicator,\n",
    "            \"CCI\": CCIIndicator,\n",
    "            \"RSI\": RSIIndicator,\n",
    "            \"VMA\": VMAIndicator,\n",
    "            \"PctChange\": PctChangeIndicator,\n",
    "            \"3M Treasury Yield\": ThreeMonthTreasuryYield,\n",
    "            \"5Y Treasury Yield\": FiveYearTreasuryYield,\n",
    "            \"10Y Treasury Yield\": TenYearTreasuryYield,\n",
    "            \"30Y Treasury Yield\": ThirtyYearTreasuryYield,\n",
    "            # Add other indicators here as needed\n",
    "        }\n",
    "        indicator = indicators.get(indicator_type)\n",
    "        if indicator is None:\n",
    "            raise ValueError(f\"Invalid indicator type: {indicator_type}\")\n",
    "        return indicator()\n",
    "\n",
    "\n",
    "class DataCleaner(ABC):\n",
    "    \"\"\"Abstract base class for data processors.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def check(self, data):\n",
    "        \"\"\"Method to check the data for issues.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def clean(self, data):\n",
    "        \"\"\"Method to clean the data from identified issues.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MissingDataCleaner(DataCleaner):\n",
    "    \"\"\"Concrete class for checking and handling missing data.\"\"\"\n",
    "\n",
    "    def check(self, data):\n",
    "        \"\"\"Check for missing data in the dataframe.\"\"\"\n",
    "        return data.isnull().sum()\n",
    "\n",
    "    def clean(self, data, strategy='auto'):\n",
    "        \"\"\"Handle missing data based on the chosen strategy.\"\"\"\n",
    "        if strategy == 'auto':\n",
    "            # Step 1: Handle missing data at the beginning\n",
    "            while data.iloc[0].isnull().any():\n",
    "                data = data.iloc[1:]\n",
    "\n",
    "            # Step 2: Fill missing data in the middle with the previous row's values\n",
    "            data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        elif strategy == 'drop':\n",
    "            data.dropna(inplace=True)\n",
    "\n",
    "        elif strategy == 'fillna':\n",
    "            data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        elif strategy == 'none':\n",
    "            # Do nothing and return the original data\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid strategy provided.\")\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class ScalerFactory:\n",
    "    \"\"\"\n",
    "    Factory class dedicated to creating scalers.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_scaler(method):\n",
    "        if method == 'StandardScaler':\n",
    "            return StandardScaler()\n",
    "        elif method == 'MinMaxScaler':\n",
    "            return MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid scaler method: {method}.\")\n",
    "\n",
    "\n",
    "class DataProcessorFactory:\n",
    "    \"\"\"Factory class to create data processors.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_cleaner(clean_type, *args, **kwargs):\n",
    "        \"\"\"Create a data processor based on the provided type.\"\"\"\n",
    "        if clean_type == \"MissingData\":\n",
    "            return MissingDataCleaner(*args, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Processor type {clean_type} not recognized.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_data(data, method='StandardScaler'):\n",
    "        \"\"\"\n",
    "        Standardize the data based on the chosen method.\n",
    "\n",
    "        Parameters:\n",
    "        - data: The data to be standardized.\n",
    "        - method: The standardization method.\n",
    "\n",
    "        Returns:\n",
    "        - The standardized data.\n",
    "        \"\"\"\n",
    "        scaler = ScalerFactory.get_scaler(method)\n",
    "        return scaler.fit_transform(data)\n",
    "\n",
    "    # add a new method that can split data by date\n",
    "    @staticmethod\n",
    "    def standardize_and_split_data(data, split_ratio=0.7, target_col=\"Trend\", feature_cols=None):\n",
    "        \"\"\"Standardize the data and split it into training and testing sets.\"\"\"\n",
    "        if not feature_cols:\n",
    "            feature_cols = data.columns.to_list()\n",
    "\n",
    "        x_data = data[feature_cols]\n",
    "\n",
    "        # Generate the one-hot encoding\n",
    "        y_data = pd.get_dummies(data[target_col], prefix='Trend')\n",
    "\n",
    "        # something wrong here\n",
    "        # # Ensure y_data has two columns\n",
    "        # missing_columns = set([\"Trend_0\", \"Trend_1\"]) - set(y_data.columns)\n",
    "        # for col in missing_columns:\n",
    "        #     y_data[col] = 0\n",
    "\n",
    "        # # Ensure columns are in the correct order\n",
    "        # y_data = y_data[[\"Trend_0\", \"Trend_1\"]]\n",
    "\n",
    "        # Check if the split index is valid\n",
    "        split_idx = int(len(x_data) * split_ratio)\n",
    "        if split_idx < 1 or split_idx >= len(x_data):\n",
    "            raise ValueError(\n",
    "                \"Invalid split ratio leading to incorrect data partitioning.\")\n",
    "\n",
    "        X_test = x_data.iloc[split_idx:]\n",
    "        y_test = y_data.iloc[split_idx:]\n",
    "        X_train = x_data.iloc[:split_idx]\n",
    "        y_train = y_data.iloc[:split_idx]\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_multistep_data(x_data, y_data, look_back, predict_steps, slide_steps=1):\n",
    "        \"\"\"\n",
    "        Prepare the data for multi-step prediction and apply standardization within each sliding window.\n",
    "        \"\"\"\n",
    "        x_date = []\n",
    "        y_date = []\n",
    "        x_data_multistep = []\n",
    "        y_data_multistep = []\n",
    "\n",
    "        for i in range(0, len(x_data) - look_back - predict_steps + 1, slide_steps):\n",
    "            x_date.append(x_data.index[i:i + look_back])\n",
    "\n",
    "            # For y_date extraction\n",
    "            y_date.append(\n",
    "                x_data.index[i + look_back:i + look_back + predict_steps])\n",
    "\n",
    "            # Extract data for the current window\n",
    "            x_window = x_data.iloc[i:i + look_back].values\n",
    "            y_window = y_data.iloc[i + look_back:i +\n",
    "                                   look_back + predict_steps].values\n",
    "\n",
    "            # Standardize the data within the window\n",
    "            scaler_x = StandardScaler()\n",
    "            x_window_standardized = scaler_x.fit_transform(x_window)\n",
    "\n",
    "            x_data_multistep.append(x_window_standardized)\n",
    "            y_data_multistep.append(y_window)\n",
    "\n",
    "        return np.array(x_data_multistep), np.array(y_data_multistep), np.array(x_date), np.array(y_date)\n",
    "\n",
    "\n",
    "# Step 4: The ModelDataAPI using Factory Pattern\n",
    "class ModelDataAPI:\n",
    "    def __init__(self, data=None, start_date=None, end_date=None):\n",
    "        self.data = data\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.trend_method = \"MA\"\n",
    "        self.indicators = []\n",
    "        self.processors = []\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def set_seed(self, seed_value=42):\n",
    "        \"\"\"Set seed for reproducibility.\"\"\"\n",
    "        np.random.seed(seed_value)\n",
    "\n",
    "    def fetch_stock_data(self, stock_symbol, start_date=None, end_date=None):\n",
    "        \"\"\"Fetch stock data from Yahoo Finance.\"\"\"\n",
    "        if start_date:\n",
    "            self.start_date = start_date\n",
    "        if end_date:\n",
    "            self.end_date = end_date\n",
    "        return yf.download(stock_symbol, start=self.start_date, end=self.end_date)\n",
    "\n",
    "    def add_indicator(self, indicator_type, *args, **kwargs):\n",
    "        indicator = IndicatorFactory.get_indicator(indicator_type)\n",
    "        self.data = indicator.compute(self.data, *args, **kwargs)\n",
    "\n",
    "    def add_data_cleaner(self, clean_type='MissingData', strategy='drop'):\n",
    "        \"\"\"Method to check and clean the data using a specific processor.\"\"\"\n",
    "        processor = DataProcessorFactory.create_cleaner(clean_type)\n",
    "        issues = processor.check(self.data)\n",
    "        self.data = processor.clean(self.data, strategy=strategy)\n",
    "        return issues\n",
    "\n",
    "    def process_data(self, split_ratio=0.7, target_col=\"Trend\", feature_cols=None, look_back=64, predict_steps=16, train_slide_steps=1, test_slide_steps=16):\n",
    "        \"\"\"\n",
    "        Use DataProcessorFactory to standardize and split the data, and prepare it for multi-step prediction if required.\n",
    "        \"\"\"\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = DataProcessorFactory.standardize_and_split_data(\n",
    "            self.data, split_ratio, target_col, feature_cols)\n",
    "\n",
    "        if look_back and predict_steps:\n",
    "            self.X_train, self.y_train, self.train_dates, _ = DataProcessorFactory.prepare_multistep_data(\n",
    "                self.X_train, self.y_train, look_back, predict_steps, train_slide_steps)\n",
    "            self.X_test, self.y_test, _, self.test_dates = DataProcessorFactory.prepare_multistep_data(\n",
    "                self.X_test, self.y_test, look_back, predict_steps, test_slide_steps)\n",
    "\n",
    "\n",
    "# Test the ModelDataAPI with Factory Pattern\n",
    "ac = ModelDataAPI()\n",
    "ac.set_seed(42)\n",
    "start_date = \"2001-01-01\"\n",
    "stop_date = \"2021-01-01\"\n",
    "stock_symbol = \"^GSPC\"\n",
    "ac.data = ac.fetch_stock_data(stock_symbol, start_date, stop_date)\n",
    "# x = np.linspace(0, 50, 1000)  # Generate 1000 points between 0 and 50\n",
    "# sin_wave = np.sin(x)  # Generate a sinusoidal wave\n",
    "# ac.data = pd.DataFrame({\n",
    "#     'Open': sin_wave,\n",
    "#     'High': sin_wave + 0.1,  # Adding a small value to simulate the 'High' value for the day\n",
    "#     'Low': sin_wave - 0.1,  # Subtracting a small value to simulate the 'Low' value for the day\n",
    "#     'Close': sin_wave,\n",
    "#     'Trend': [0 for i in range(1000)]\n",
    "#     # 'Trend': [(i % 2) for i in range(1000)]\n",
    "# })\n",
    "\n",
    "indicators = [\n",
    "    {\"type\": \"Trend\", \"method\": \"MA\", \"oder_days\": 20,\n",
    "        \"ma_days\": 20, \"trend_days\": 5},\n",
    "    {\"type\": \"MACD\", \"fastperiod\": 5, \"slowperiod\": 10, \"signalperiod\": 9},\n",
    "    {\"type\": \"ROC\", \"trend_days\": 5},\n",
    "    {\"type\": \"Stochastic Oscillator\", \"trend_days\": 5},\n",
    "    {\"type\": \"CCI\", \"timeperiod\": 14},\n",
    "    {\"type\": \"RSI\", \"timeperiod\": 14},\n",
    "    {\"type\": \"VMA\", \"timeperiod\": 20},\n",
    "    {\"type\": \"PctChange\"},\n",
    "    {\"type\": \"3M Treasury Yield\", \"start_date\": \"2001-01-01\", \"end_date\": \"2021-01-01\"},\n",
    "    {\"type\": \"5Y Treasury Yield\", \"start_date\": \"2001-01-01\", \"end_date\": \"2021-01-01\"},\n",
    "    {\"type\": \"10Y Treasury Yield\", \"start_date\": \"2001-01-01\", \"end_date\": \"2021-01-01\"},\n",
    "    {\"type\": \"30Y Treasury Yield\", \"start_date\": \"2001-01-01\", \"end_date\": \"2021-01-01\"},\n",
    "]  # Add other indicators here as needed\n",
    "\n",
    "for indicator_params in indicators:\n",
    "    indicator_type = indicator_params[\"type\"]\n",
    "    ac.add_indicator(indicator_type, **indicator_params)\n",
    "\n",
    "issues_detected = ac.add_data_cleaner(\"MissingData\", strategy='auto')\n",
    "\n",
    "# Specify data processing parameters\n",
    "split_ratio = 0.7\n",
    "target_col = \"Trend\"\n",
    "# feature_cols = None  # None means use all columns\n",
    "feature_cols = ['Close']\n",
    "look_back = 64  # number of previous days' data to consider\n",
    "predict_steps = 16  # number of days to predict in the future\n",
    "slide_steps = 1  # sliding window step size\n",
    "\n",
    "# Call the process_data method to standardize and split the data\n",
    "ac.process_data(split_ratio=0.7, target_col=\"Trend\", feature_cols=feature_cols, look_back=look_back,\n",
    "                predict_steps=predict_steps, train_slide_steps=1, test_slide_steps=predict_steps)\n",
    "\n",
    "# Display the shapes of the training and testing datasets\n",
    "ac.X_train.shape, ac.y_train.shape, ac.X_test.shape, ac.y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open                  0\n",
       "High                  0\n",
       "Low                   0\n",
       "Close                 0\n",
       "Adj Close             0\n",
       "Volume                0\n",
       "Trend                 0\n",
       "MACD                  0\n",
       "ROC                   0\n",
       "StoK                  0\n",
       "StoD                  0\n",
       "CCI                   0\n",
       "RSI                   0\n",
       "VMA                   0\n",
       "pctChange             0\n",
       "3M Treasury Yield     0\n",
       "5Y Treasury Yield     0\n",
       "10Y Treasury Yield    0\n",
       "30Y Treasury Yield    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3430, 64, 1), (3430, 16, 2), (90, 64, 1), (90, 16, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify data processing parameters\n",
    "split_ratio = 0.7\n",
    "target_col = \"Trend\"\n",
    "# feature_cols = None  # None means use all columns\n",
    "feature_cols = ['Close']\n",
    "look_back = 64  # number of previous days' data to consider\n",
    "predict_steps = 16  # number of days to predict in the future\n",
    "slide_steps = 1  # sliding window step size\n",
    "\n",
    "# Call the process_data method to standardize and split the data\n",
    "ac.process_data(split_ratio=0.7, target_col=\"Trend\", feature_cols=feature_cols, look_back=look_back,\n",
    "                predict_steps=predict_steps, train_slide_steps=1, test_slide_steps=predict_steps)\n",
    "\n",
    "# Display the shapes of the training and testing datasets\n",
    "ac.X_train.shape, ac.y_train.shape, ac.X_test.shape, ac.y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trend_0': 3252, 'Trend_1': 1761}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the last dimension and calculating the ratio of the two classes in ac.y_train\n",
    "class_0_count = np.sum(ac.data['Trend'] == 0)\n",
    "class_1_count = np.sum(ac.data['Trend'] == 1)\n",
    "\n",
    "class_ratio = {\n",
    "    \"Trend_0\": class_0_count,\n",
    "    \"Trend_1\": class_1_count\n",
    "}\n",
    "\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trend_0': 34900, 'Trend_1': 19980}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the last dimension and calculating the ratio of the two classes in ac.y_train\n",
    "class_0_count = np.sum(ac.y_train[:, :, 0])\n",
    "class_1_count = np.sum(ac.y_train[:, :, 1])\n",
    "\n",
    "class_ratio = {\n",
    "    \"Trend_0\": class_0_count,\n",
    "    \"Trend_1\": class_1_count\n",
    "}\n",
    "\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trend_0': 1008, 'Trend_1': 432}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the last dimension and calculating the ratio of the two classes in ac.y_train\n",
    "class_0_count = np.sum(ac.y_test[:, :, 0])\n",
    "class_1_count = np.sum(ac.y_test[:, :, 1])\n",
    "\n",
    "class_ratio = {\n",
    "    \"Trend_0\": class_0_count,\n",
    "    \"Trend_1\": class_1_count\n",
    "}\n",
    "\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E..\n",
      "======================================================================\n",
      "ERROR: test_data_splitting (__main__.TestDataAPI)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\e4903\\AppData\\Local\\Temp\\ipykernel_1608\\3007816089.py\", line 81, in test_data_splitting\n",
      "    self.ac.process_data(split_ratio=split_ratio, target_col=target_col,\n",
      "TypeError: ModelDataAPI.process_data() got an unexpected keyword argument 'slide_steps'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.469s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=1 failures=0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestDataAPI(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.ac = ModelDataAPI()\n",
    "        self.ac.set_seed(42)\n",
    "        self.start_date = \"2020-01-01\"\n",
    "        self.end_date = \"2021-01-01\"\n",
    "        self.stock_symbol = \"^GSPC\"\n",
    "\n",
    "        # Mock data for the tests to avoid yfinance dependency\n",
    "        x = np.linspace(0, 50, 1000)  # Generate 1000 points between 0 and 50\n",
    "        sin_wave = np.sin(x)  # Generate a sinusoidal wave\n",
    "        self.ac.data = pd.DataFrame({\n",
    "            'Open': sin_wave,\n",
    "            'High': sin_wave + 0.1,  # Adding a small value to simulate the 'High' value for the day\n",
    "            # Subtracting a small value to simulate the 'Low' value for the day\n",
    "            'Low': sin_wave - 0.1,\n",
    "            'Close': sin_wave,\n",
    "            'Trend': [0 for i in range(1000)]\n",
    "            # 'Trend': [(i % 2) for i in range(1000)]\n",
    "        })\n",
    "\n",
    "    def test_fetch_stock_data(self):\n",
    "        self.assertIsNotNone(self.ac.data)\n",
    "        self.assertFalse(self.ac.data.empty)\n",
    "\n",
    "    def test_add_single_indicator(self):\n",
    "        initial_columns = set(self.ac.data.columns)\n",
    "        self.ac.add_indicator(\"MACD\", fastperiod=5,\n",
    "                              slowperiod=10, signalperiod=9)\n",
    "        new_columns = set(self.ac.data.columns)\n",
    "        self.assertGreater(len(new_columns), len(initial_columns))\n",
    "\n",
    "    def test_add_multiple_indicators(self):\n",
    "        initial_columns = set(self.ac.data.columns)\n",
    "        self.ac.add_indicator(\"RSI\", timeperiod=14)\n",
    "        self.ac.add_indicator(\"CCI\", timeperiod=14)\n",
    "        new_columns = set(self.ac.data.columns)\n",
    "        self.assertGreater(len(new_columns), len(initial_columns))\n",
    "\n",
    "    def test_invalid_indicator(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.ac.add_indicator(\"InvalidIndicatorName\")\n",
    "\n",
    "    def test_data_cleaning(self):\n",
    "        # Introducing missing values into the mock data\n",
    "        self.ac.data.iloc[2, 1] = np.nan\n",
    "        self.ac.data.iloc[4, 3] = np.nan\n",
    "        initial_missing_count = self.ac.data.isnull().sum().sum()\n",
    "\n",
    "        # Use the add_data_cleaner method to clean the data\n",
    "        self.ac.add_data_cleaner(\"MissingData\", strategy='auto')\n",
    "\n",
    "        # Verify that missing values have been cleaned\n",
    "        final_missing_count = self.ac.data.isnull().sum().sum()\n",
    "        self.assertLess(final_missing_count, initial_missing_count)\n",
    "\n",
    "    def test_data_splitting(self):\n",
    "        # Populate the mock data with necessary indicators\n",
    "        indicators = [\n",
    "            {\"type\": \"Trend\", \"method\": \"MA\", \"oder_days\": 20,\n",
    "                \"ma_days\": 20, \"trend_days\": 5},\n",
    "            # Add other necessary indicators here\n",
    "        ]\n",
    "\n",
    "        for indicator_params in indicators:\n",
    "            indicator_type = indicator_params[\"type\"]\n",
    "            self.ac.add_indicator(indicator_type, **indicator_params)\n",
    "\n",
    "        # Call the process_data method to standardize and split the data\n",
    "        split_ratio = 0.7\n",
    "        target_col = \"Trend\"\n",
    "        feature_cols = None\n",
    "        look_back = 10\n",
    "        predict_steps = 5\n",
    "        slide_steps = 1\n",
    "\n",
    "        self.ac.process_data(split_ratio=split_ratio, target_col=target_col,\n",
    "                             look_back=look_back, predict_steps=predict_steps, slide_steps=slide_steps)\n",
    "\n",
    "        # Test the shape of the training and testing datasets\n",
    "        self.assertEqual(self.ac.X_train.shape[0], self.ac.y_train.shape[0])\n",
    "        self.assertEqual(self.ac.X_test.shape[0], self.ac.y_test.shape[0])\n",
    "\n",
    "        # Ensure sum of lengths of train and test datasets matches the length of the original dataset\n",
    "        total_data_points = len(self.ac.data) - look_back - predict_steps + 1\n",
    "        self.assertEqual(\n",
    "            self.ac.X_train.shape[0] + self.ac.X_test.shape[0], total_data_points)\n",
    "\n",
    "        # Ensure data split ratio is approximately maintained\n",
    "        train_ratio = self.ac.X_train.shape[0] / total_data_points\n",
    "        self.assertAlmostEqual(train_ratio, split_ratio, places=1)\n",
    "\n",
    "\n",
    "# Running the tests\n",
    "unittest_result_splitting = unittest.TextTestRunner().run(\n",
    "    unittest.TestLoader().loadTestsFromTestCase(TestDataAPI))\n",
    "unittest_result_splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
